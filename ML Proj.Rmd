---
title: "Final Project for Johns Hopkins Machine Learning"
author: "Edward Russell"
date: ""
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	results = "hide",
	tidy = TRUE
)
```
 <br/>
 <br/>
<h3>Using Machine Learning to Make Predictions From Biometric Data</h3>  


  
The first thing we to do is download the training data. Next we load the data and have a look at it and notice that there are a number of columns that contain almost all NAs and a number of columns that are virtually all blank space. Since there is very little data in those columns and this is a relatively large dataset we can just remove the offending columns. We will also remove the first 7 columns which do not contain useful information.
```{r, echo=TRUE, eval=FALSE}
library(RCurl)
myfile<-getURL('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv',
               ssl.verifyhost=FALSE, ssl.verifypeer=FALSE)
pml<- read.csv(textConnection(myfile), header = TRUE)
View(pml)
pml2<- pml[,colSums(is.na(pml))==0] # remove columns with NAs
pml2<- pml2[,!apply(pml2, 2, function(x) any(x==""))] # remove columns with blank entries
View(pml2)
pml2<-pml2[,8:60]
```
  <br/>
  <br/>
Now that we have our data cleaned up we can start to choose which of the potential features present in the dataset we are going to use to build our benchmark models. We will use these models to compare between one another to decide what model(s) we want to use later on. To do this we need to load the caret library and then set control attributes and carry out recursive feature elimination to choose features to be used by each of our candidate models. Notice that we specify 4-fold cross validation in the control attributes to counteract any overfitting due to feature selection.
```{r echo=TRUE, eval=FALSE}
library(caret)
control1 <- rfeControl(functions=rfFuncs, method="cv", number=4)
control2 <- rfeControl(functions=treebagFuncs, method="cv", number=4)
control3<- rfeControl(functions = ldaFuncs, method = "cv", number = 4)

system.time(
  results1 <- rfe(pml2[,names(pml2)!="classe"], pml2[,"classe"],
                sizes=c(1:15), rfeControl=control1)
)
system.time(
  results2 <- rfe(pml2[,names(pml2)!="classe"], pml2[,"classe"],
                sizes=c(1:15), rfeControl=control2)
)
system.time(
  results3 <- rfe(pml2[,names(pml2)!="classe"], pml2[,"classe"],
                sizes=c(1:15), rfeControl=control3)
)
```
  <br/>
  <br/>
Now we can look at plots of the predictors, or features, indicated by the result of our recursive feature elimination calls. Only the first two lines' output are included for illustration.
```{r eval=FALSE, echo=TRUE}
predictors(results1) # in order of importance
plot(results1, type=c("g", "o")) # how many features do we need? (13-14+)

predictors(results2)
plot(results2, type=c("g", "o")) # 14+ features looks good

predictors(results3)
plot(results3, type=c("g", "o")) # all features give ~ 0.7 accuracy :(
```
  <br/>
  <br/>
In order to compare the accuracy of our models (rf, treebag, lda, and gbm) we need to train what I call benchmark models using the features indicated by our previous feature elimination efforts. I chose rf and treebag because they are high performers on datasets like the one we were provided. I also included an lda model for general illustrative purposes, and added a gradient boosting machine (gbm) model because they frequently perform well also. Since most consider gbm models to self select features I did not carry out RFE for the gbm model. So, we are trying 2 bagged decision tree approaches, a linear discriminant approach, and a boosted decision tree approach.
```{r eval=FALSE, echo=TRUE}
# Now we train some models to assess in-sample accuracy -
# using the features we selected above 
control <- trainControl(method="cv", number=5) #using 5-fold cross validation
bench1 <- train(classe~.,
                data=pml2[,c(predictors(results1)[1:14],"classe")],
                method="rf", preProcess=c("center", "scale"),
                trControl=control)
importance1 <- varImp(bench1, scale=FALSE)
print(importance1)
plot(importance1) #compare variable importance visually
a1<-predict(bench1, newdata = pml2[,names(pml2)!="classe"])
mean(a1==pml2[,"classe"]) # rough in-sample accuracy ( ~ 0.999 )
confusionMatrix(data = a1, reference = pml2$classe) #view by-class accuracy

bench2 <- train(classe~.,
                data=pml2[,c(predictors(results2)[1:14],"classe")],
                method="treebag", preProcess=c("center", "scale"),
                trControl=control)
importance2 <- varImp(bench2, scale=FALSE)
print(importance2)
plot(importance2)
a2<-predict(bench2, newdata = pml2[,names(pml2)!="classe"])
mean(a2==pml2[,"classe"]) # ~ 0.999
confusionMatrix(data = a2, reference = pml2$classe)

bench3 <- train(classe~.,
                data=pml2[,c(predictors(results3),"classe")],
                method="lda", preProcess=c("center", "scale"),
                trControl=control)
importance3 <- varImp(bench3, scale=FALSE)
print(importance3)
plot(importance3)
a3<-predict(bench3, newdata = pml2[,names(pml2)!="classe"])
mean(a3==pml2[,"classe"]) # ~ 0.705
confusionMatrix(data = a3, reference = pml2$classe)

bench4 <- train(classe~.,
               data=pml2[,c(predictors(results2)[1:15],"classe")],
               method="gbm", preProcess=c("center", "scale"),
               trControl=control)
importance4 <- varImp(bench4, scale=FALSE)
print(importance4)
plot(importance4)
a4<-predict(bench4, newdata = pml2[,names(pml2)!="classe"])
mean(a4==pml2[,"classe"]) # ~ 0.961
confusionMatrix(data = a4, reference = pml2$classe)

```
We now have some indication of the in-sample accuracy of the four models we've tried, but we need an estimate of out of sample accuracy as well. I also decided to try and stack the three best performing candidates, namely rf, treebag, and gbm. So, we must break up our training dataset into three parts. Different people call them different things, but I will call them training, validation, and testing sets. This splitting is accomplished randomly to ensure that there are sufficient rows in each set with all of the possible outcome values. This in necessary since the original data has large runs of ascending rows with only a single outcome level.
```{r eval=FALSE, echo=TRUE}
inTrain<-createDataPartition(pml2$classe, times = 1, p=0.80)[[1]]
TrainSet<-pml2[inTrain,]
ValidSet0<-pml2[-inTrain,]
innValid<-createDataPartition(ValidSet0$classe, times = 1, p=0.5)[[1]]
ValidSet<-ValidSet0[innValid,]
TestSet<-ValidSet0[-innValid,]
any(ValidSet %in% TrainSet) # check for overlap, there is none
any(TestSet %in% TrainSet) # same as above
any(TestSet %in% ValidSet) # same as above

```
<br/>
<br/>
Now that we have partitioned the data let's use the new training and validation sets to estimate the out of sample accuracy of our models. As with the benchmark models we will use cross validation for comparability and for generalizability via avoidance of overfitting and high variance. Also, recall that as the number of folds used for CV increases bias decreases as variance increases. I tried values between 2 and 10 and found that 5 was close to optimal.
```{r eval=FALSE, echo=TRUE}
control <- trainControl(method="cv", number=5) # again, we use 5-fold CV

# now we train new models on the 'new' training set and we test for
# something akin to out of sample accuracy using the validation set
model1 <- train(classe~.,
                data=TrainSet[,c(predictors(results1)[1:14],"classe")],
                method="rf", preProcess=c("center", "scale"),
                trControl=control)
# Here we make predictions on the validation set using the model
# we just trained on the training set
ValidSet$rf_preds<-predict(model1,
                           newdata = ValidSet[,names(ValidSet)!="classe"])
mean(ValidSet$rf_preds==ValidSet$classe) # akin to out of sample accuracy (~0.990)
confusionMatrix(ValidSet$rf_preds,ValidSet$classe)
# for stacking, we create predictions from the 'test' set
TestSet$rf_preds<-predict(model1,
                             newdata = TestSet[,names(TestSet)!="classe"])

model2 <- train(classe~.,
                data=TrainSet[,c(predictors(results2)[1:14],"classe")],
                method="treebag", preProcess=c("center", "scale"),
                trControl=control)
ValidSet$treebag_preds<-predict(model2,
                                newdata = ValidSet[,names(ValidSet)!="classe"])
mean(ValidSet$treebag_preds==ValidSet$classe) # akin to out of sample accuracy (~.985)
confusionMatrix(ValidSet$treebag_preds,ValidSet$classe)
TestSet$treebag_preds<-predict(model2,
                                  newdata = TestSet[,names(TestSet)!="classe"])

model3 <- train(classe~.,
                data=pml2[,c(predictors(results3),"classe")],
                method="lda", preProcess=c("center", "scale"),
                trControl=control)
ValidSet$lda_preds<-predict(model3,
                            newdata = ValidSet[,names(ValidSet)!="classe"])
mean(ValidSet$lda_preds==ValidSet$classe) # akin to out of sample accuracy (~.698)
confusionMatrix(ValidSet$lda_preds,ValidSet$classe)
TestSet$lda_preds<-predict(model3,
                              newdata = TestSet[,names(TestSet)!="classe"])

model4 <- train(classe~.,
                data=pml2[,c(predictors(results2)[1:20],"classe")],
                method="gbm", preProcess=c("center", "scale"),
                trControl=control)
ValidSet$gam_preds<-predict(model4,
                            newdata = ValidSet[,names(ValidSet)!="classe"])
mean(ValidSet$gam_preds==ValidSet$classe) # akin to out of sample accuracy (~.971)
confusionMatrix(ValidSet$gam_preds,ValidSet$classe)
TestSet$gam_preds<-predict(model4,
                              newdata = TestSet[,names(TestSet)!="classe"])
```
<br/>
<br/>
Our estimates of out of sample accuracy weren't that far off from our in-sample estiamtes of error. That is not too surprising considering we intentionally used feature elimination and cross validation to avoid overfitting. Of course, we also employed boosting and bagging as well in the gbm and tree based methods respectively. Also note that as we fit these models we also added their predictions as additional columns to the validation and 'testing' datasets. This can allow us to build either a blended model that uses features from the datasets along with the predictions, or a stacked model that uses only the predictions themselves. To use only the predictions themselves they must be pulled into a dedicated data frame. Since this was the manual ensembling technique presented by Dr. Peng I chose to try this first. Also, I chose to leave out the lda model as it was the poorest performer. 
```{r eval=FALSE, echo=TRUE}
# Next we create a dataframe with just the validation set predictions
# from all of the models
PredsDF_V<-data.frame(rfPreds = ValidSet$rf_preds,
                    treebagPreds = ValidSet$treebag_preds,
                    #ldaPreds = ValidSet$lda_preds,
                    gbmPreds = ValidSet$gam_preds,
                    classe = ValidSet$classe)

# We train a model on the predictions in the dataframe populated
# with only predictions based on the validation set
StackedMod<- train(classe~.,data = PredsDF_V, method = "rf",
                   preProcess=c("center", "scale"),
                   trControl = control)

# We create a dataframe with just the 'test' set predictions
# made by applying the models trained on the training set
PredsDF_T<-data.frame(rfPreds = TestSet$rf_preds,
                      treebagPreds = TestSet$treebag_preds,
                      #ldaPreds = TestSet$lda_preds,
                      gbmPreds = TestSet$gam_preds,
                      classe = TestSet$classe)
# We make predictions using the stacked model on the 'test' set
# predictions, which were made by the models trained on the training
# set only
TestPreds<- predict(StackedMod, PredsDF_T)
# better assessment of the out of sample accuracy (~0.986)
mean(TestPreds==TestSet$classe)
confusionMatrix(TestPreds,TestSet$classe)
```
<br/>
<br/>
Unfortunately, our accuracy with the stacked model was not better that the rf model alone! First we should check the accuracy of the individual models on our 'new' validation dataset. Also, I was curious how a simple majority vote based on the predictions of the three individual models would compare to our stacked model. 
```{r, echo=TRUE, eval=FALSE}
#compare to individual model accuracy
mean(PredsDF_T$rfPreds==PredsDF_T$classe) # ~ 0.991
mean(PredsDF_T$treebagPreds==PredsDF_T$classe) # ~ 0.980
mean(PredsDF_T$gbmPreds==PredsDF_T$classe) # ~ 0.958

# to do a simple majority vote, first we define a Mode function
Mode <- function(x) {
ux <- unique(x)
ux[which.max(tabulate(match(x, ux)))]
}

for(i in 1:nrow(PredsDF_T)){
  PredsDF_T$maj[i]<-as.character(Mode(PredsDF_T[i,1:3])[[1]])
}

mean(PredsDF_T$classe==PredsDF_T$maj) # ~ 0.984, boo!
```

So, it seems that we should perhaps try something else. Since ensembling frequently increases the accuracy above that of a single model, I decided to evaluate a blended model that only incorporated the rf and the treebag models, as they had the highest estimates of out of sample error. To do this we want to verify our choice of features for the rf model, since it is the highest performer. We will also create a new data frame that contains the features that we have chosen as being of high importance to both the rf and the treebag models.
```{r eval=FALSE, echo=TRUE}
# this first code chunk verifies that ~14+ is a good number of predictors for rf
result<-rfcv(pml2[,names(pml2)!="classe"],pml2$classe, cv.fold = 10)
with(result, plot(n.var, error.cv, log="x", type="o", lwd=2))
# I'm selecting the 18 features that occur in the top 20 features previously
# determined for each of the top two models
intersect(predictors(results1)[1:20],predictors(results2)[1:20])
feats<-intersect(predictors(results1)[1:20],predictors(results2)[1:20])
pml3<-pml2[,c(feats,"classe")] # new dataset that contains only those features
```
<br/>
<br/>
Next we cut the dataset into three random parts as we did before when we were stacking the rf, treebag, and gbm models.
```{r echo=TRUE, eval=FALSE}
inTrain<-createDataPartition(pml3$classe, times = 1, p=0.80)[[1]]
TrainSet<-pml3[inTrain,]
ValidSet0<-pml3[-inTrain,]
innValid<-createDataPartition(ValidSet0$classe, times = 1, p=0.5)[[1]]
ValidSet<-ValidSet0[innValid,]
TestSet<-ValidSet0[-innValid,]
any(ValidSet %in% TrainSet) # check for overlap, there is none
any(TestSet %in% TrainSet) # same as above
any(TestSet %in% ValidSet) # same as above
```
<br/>
<br/>
Just like we did with the individual models that we used to create the stacked model, we will estimate out of sample error and then compare those values to the accuracy of the final ensembled model. The code is similar to that for the stacked except that we are using all of the predictors used to trian the component models (18 available in the data sets we're using for this purpose) and the predictions those models made all together to train the final blended model. Also, we've used 10-fold CV in the model training as I found heuristically that this was close to optimal.
```{r echo = TRUE, eval=FALSE}
control <- trainControl(method="cv", number=10) # here we'll use 10-fold CV

# now we train new models on the new 'new' training set and we test for
# something akin to out of sample accuracy using the validation set
model1 <- train(classe~.,
                data=TrainSet,
                method="rf", preProcess=c("center","scale"),
                trControl=control)
# Here we make predictions on the validation set using the model
# we just trained on the training set so we can estimate out of
# sample error
ValidSet$rf_preds<-predict(model1,
                           newdata = ValidSet[,feats])
mean(ValidSet$rf_preds==ValidSet$classe) # akin to out of sample accuracy (~0.990)
confusionMatrix(ValidSet$rf_preds,ValidSet$classe)
# for blending, we create predictions from the 'test' set to be added as
# new column(s) to the 'test' set
TestSet$rf_preds<-predict(model1,
                          newdata = TestSet[,feats])

model2 <- train(classe~.,
                data=TrainSet,
                method="treebag", preProcess=c("center","scale"),
                trControl=control)
ValidSet$treebag_preds<-predict(model2,
                                newdata = ValidSet[,feats])
mean(ValidSet$treebag_preds==ValidSet$classe) #                           (~0.985)
confusionMatrix(ValidSet$treebag_preds,ValidSet$classe)
TestSet$treebag_preds<-predict(model2,
                               newdata = TestSet[,feats])
# Here we train the blended model on the 'validation' set which we've added
# columns of predictions to from the two component models
BlendMod<- train(classe~., data = ValidSet, method = "rf",
                   preProcess =c("center","scale"),
                   trControl = control)
StackedPreds<- predict(BlendMod, TestSet[,names(TestSet)!="classe"])
mean(StackedPreds==TestSet$classe) # (~0.994) akin to out of sample accuracy for our
# final, blended model (better than either model alone, which was not the case when 
# we trained a stacked model on the predictions from the other initial models alone)
```
**So, as you can see the accuracy of the blended model was greater that the accuracy of either of the component models. The accuracy for the final blended model was ~ 0.994.** That value varied from 0.992 to 0.996 depending on the initial seed for random number generation. In conclusion, we have produced an ensembled model that we've estimated to have an out of sample accuracy of approximately 0.994, which is superior to any of the individual models tested both in what is shown here and others which were not shown for the sake of brevity.
<br/>
<br/>
<br/>
